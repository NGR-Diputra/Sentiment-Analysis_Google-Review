{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m2RcDGoZiAEk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from flask import Flask, render_template, request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FTsZT8u4EIuV"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"dataset.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7MSUoV3zD9_"
      },
      "source": [
        "=== PREPROCESSING TEXT ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZawxDtfCzjDA",
        "outputId": "15b86e1b-8952-4366-cc8f-6269358fb644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: Sastrawi in c:\\users\\diputra_w\\appdata\\roaming\\python\\python311\\site-packages (1.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install Sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VhpKayX3sQZD"
      },
      "outputs": [],
      "source": [
        "#https://chat.openai.com/share/6da81ec8-3cd1-48d6-9ee1-e29202cfe7c5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mIFf-NTDFR2A"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecqgZiH5E3rO",
        "outputId": "d85ab6df-9b24-4f59-ddd8-35996106b3cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Diputra_W\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JBcq5ws4z3_4"
      },
      "outputs": [],
      "source": [
        "def case_folding(sentence):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # Emojis in the first range\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # Emojis in the second range\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # Emojis in the third range\n",
        "        u\"\\U0001F700-\\U0001F77F\"  # Emojis in the fourth range\n",
        "        u\"\\U0001F780-\\U0001F7FF\"  # Emojis in the fifth range\n",
        "        u\"\\U0001F800-\\U0001F8FF\"  # Emojis in the sixth range\n",
        "        u\"\\U0001F900-\\U0001F9FF\"  # Emojis in the seventh range\n",
        "        u\"\\U0001FA00-\\U0001FA6F\"  # Emojis in the eighth range\n",
        "        u\"\\U0001FA70-\\U0001FAFF\"  # Emojis in the ninth range\n",
        "        u\"\\U0001F004-\\U0001F0CF\"  # Emojis in the tenth range\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "\n",
        "    sentence = emoji_pattern.sub(r'', sentence)\n",
        "    sentence = sentence.translate(str.maketrans(\"\",\"\", string.punctuation)).lower()\n",
        "    sentence = re.sub(r\"\\d+\", \"\", sentence)\n",
        "    sentence = sentence.replace(\"/\", \" \")\n",
        "    return sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4k0eRmAo0NgI"
      },
      "outputs": [],
      "source": [
        "def load_abbreviation_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, \"r\") as file:\n",
        "            abbreviations = json.load(file)\n",
        "        return abbreviations\n",
        "    except FileExistsError:\n",
        "        print(f\"File not found {file_path}\")\n",
        "        return {}\n",
        "\n",
        "#Reading the abbreviation file path for preprocessing\n",
        "file_path = \"abbreviation_file.txt\"\n",
        "abbreviation_file = load_abbreviation_file(file_path)\n",
        "\n",
        "def normalize_text(sentence):\n",
        "    words = sentence.lower().split()\n",
        "    words_normalized = []\n",
        "    for word in words:\n",
        "        for full_form, abbreviations in abbreviation_file.items():\n",
        "            if word.lower() in abbreviations:\n",
        "                words_normalized.append(full_form)\n",
        "                break\n",
        "        else:\n",
        "            words_normalized.append(word)\n",
        "    return \" \".join(words_normalized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U2FP60DW3nHA"
      },
      "outputs": [],
      "source": [
        "def stopwords_removal(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    liststopwords =  set(stopwords.words('indonesian'))\n",
        "\n",
        "    custom_stopwords_file = \"more_stopwords.txt\"\n",
        "\n",
        "    custom_stopwords = set()\n",
        "    with open(custom_stopwords_file, \"r\") as file:\n",
        "        for line in file:\n",
        "            custom_stopwords.add(line.strip())\n",
        "\n",
        "    combined_stopwords = liststopwords.union(custom_stopwords)\n",
        "\n",
        "    with open(custom_stopwords_file, \"w\") as file:\n",
        "        for word in combined_stopwords:\n",
        "            file.write(word + \"\\n\")\n",
        "\n",
        "def remove_custom_stopwords(sentence, custom_stopwords_file):\n",
        "    custom_stopwords = set()\n",
        "    with open(custom_stopwords_file, 'r') as file:\n",
        "        for line in file:\n",
        "            custom_stopwords.add(line.strip())\n",
        "\n",
        "    words = word_tokenize(sentence)\n",
        "\n",
        "    filtered_words = [word for word in words if word.lower() not in custom_stopwords]\n",
        "\n",
        "    cleaned_text = ' '.join(filtered_words)\n",
        "\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DpBLqDQiC8w-"
      },
      "outputs": [],
      "source": [
        "def stemming_text(sentence):\n",
        "    factory = StemmerFactory()\n",
        "    Stemmer = factory.create_stemmer()\n",
        "\n",
        "    sentence = Stemmer.stem(sentence)\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "yaZmGj_zDjEP",
        "outputId": "a8c12036-5e93-4b5c-98ec-889bc176305d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author_title</th>\n",
              "      <th>review_rating</th>\n",
              "      <th>review_text</th>\n",
              "      <th>case_folding</th>\n",
              "      <th>normalized_text</th>\n",
              "      <th>stopword_removed</th>\n",
              "      <th>stemmed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Felix zhang</td>\n",
              "      <td>5</td>\n",
              "      <td>Urus KTP hilang cukup surat keterangan hilang dari kepolisian dan fotokopi KK\\nAntrian sekitar 1 jam, proses cetak hanya sekitar 2 menit</td>\n",
              "      <td>urus ktp hilang cukup surat keterangan hilang dari kepolisian dan fotokopi kk\\nantrian sekitar  jam proses cetak hanya sekitar  menit</td>\n",
              "      <td>urus kartu tanda penduduk hilang cukup surat keterangan hilang dari kepolisian dan fotokopi kartu keluarga antrian sekitar jam proses cetak hanya sekitar menit</td>\n",
              "      <td>urus kartu tanda penduduk hilang surat keterangan hilang kepolisian fotokopi kartu keluarga antrian jam proses cetak menit</td>\n",
              "      <td>urus kartu tanda duduk hilang surat terang hilang polisi fotokopi kartu keluarga antri jam proses cetak menit</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  author_title  review_rating   \n",
              "0  Felix zhang              5  \\\n",
              "\n",
              "                                                                                                                                review_text   \n",
              "0  Urus KTP hilang cukup surat keterangan hilang dari kepolisian dan fotokopi KK\\nAntrian sekitar 1 jam, proses cetak hanya sekitar 2 menit  \\\n",
              "\n",
              "                                                                                                                            case_folding   \n",
              "0  urus ktp hilang cukup surat keterangan hilang dari kepolisian dan fotokopi kk\\nantrian sekitar  jam proses cetak hanya sekitar  menit  \\\n",
              "\n",
              "                                                                                                                                                   normalized_text   \n",
              "0  urus kartu tanda penduduk hilang cukup surat keterangan hilang dari kepolisian dan fotokopi kartu keluarga antrian sekitar jam proses cetak hanya sekitar menit  \\\n",
              "\n",
              "                                                                                                             stopword_removed   \n",
              "0  urus kartu tanda penduduk hilang surat keterangan hilang kepolisian fotokopi kartu keluarga antrian jam proses cetak menit  \\\n",
              "\n",
              "                                                                                                    stemmed_text  \n",
              "0  urus kartu tanda duduk hilang surat terang hilang polisi fotokopi kartu keluarga antri jam proses cetak menit  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['case_folding'] = df['review_text'].apply(case_folding)\n",
        "df['normalized_text'] = df['case_folding'].apply(normalize_text)\n",
        "df['stopword_removed'] = df['normalized_text'].apply(lambda x: remove_custom_stopwords(x, 'more_stopwords.txt'))\n",
        "df['stemmed_text'] = df['stopword_removed'].apply(stemming_text)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "hRP1EfCoJZ_I",
        "outputId": "6fb4f6c5-5eb1-4657-fa8b-f3553f04b711"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stemmed_text</th>\n",
              "      <th>review_rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>urus kartu tanda duduk hilang surat terang hilang polisi fotokopi kartu keluarga antri jam proses cetak menit</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>urus akta nikah online taring proses cetak arah kantor informasi terima email cetak akta paket kartu tanda duduk kartu keluarga terbit admin taring respon tanggap cepat respect admin suasana dukcapil loket layan cepat ramah tunggu nyaman antri ramai paham alur mudah cepat</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>layan mana buat suratsurat mudah sistem online bulakbalik penuh dokumen langsung cetak mandiri bawa ojek online tugas ramah bimbing sulit moga layan tahan tingkat terimakasih</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gaji staf staf dukcapil kerja buru buru pulang jam salah salah cetak kartu tanda duduk kartu keluarga status agama kartu tanda duduk ganti buru buru pulang kerja beres</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>proses cepat online riweh bingung tinggal admin layan ramah ya beda sangtta yaa sih ter the best pokok</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                       stemmed_text   \n",
              "0                                                                                                                                                                     urus kartu tanda duduk hilang surat terang hilang polisi fotokopi kartu keluarga antri jam proses cetak menit  \\\n",
              "1  urus akta nikah online taring proses cetak arah kantor informasi terima email cetak akta paket kartu tanda duduk kartu keluarga terbit admin taring respon tanggap cepat respect admin suasana dukcapil loket layan cepat ramah tunggu nyaman antri ramai paham alur mudah cepat   \n",
              "2                                                                                                    layan mana buat suratsurat mudah sistem online bulakbalik penuh dokumen langsung cetak mandiri bawa ojek online tugas ramah bimbing sulit moga layan tahan tingkat terimakasih   \n",
              "3                                                                                                           gaji staf staf dukcapil kerja buru buru pulang jam salah salah cetak kartu tanda duduk kartu keluarga status agama kartu tanda duduk ganti buru buru pulang kerja beres   \n",
              "4                                                                                                                                                                            proses cepat online riweh bingung tinggal admin layan ramah ya beda sangtta yaa sih ter the best pokok   \n",
              "\n",
              "   review_rating  \n",
              "0              5  \n",
              "1              5  \n",
              "2              5  \n",
              "3              1  \n",
              "4              5  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfnew = df[[\"stemmed_text\", \"review_rating\"]]\n",
        "dfnew.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lfytYGd8KISV"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "AAJCM4KoR8kZ",
        "outputId": "d5761055-a2c9-4dc9-f931-1dc68b6c04a6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stemmed_text</th>\n",
              "      <th>sentiment_category</th>\n",
              "      <th>encoded_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>urus kartu tanda duduk hilang surat terang hilang polisi fotokopi kartu keluarga antri jam proses cetak menit</td>\n",
              "      <td>Positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>urus akta nikah online taring proses cetak arah kantor informasi terima email cetak akta paket kartu tanda duduk kartu keluarga terbit admin taring respon tanggap cepat respect admin suasana dukcapil loket layan cepat ramah tunggu nyaman antri ramai paham alur mudah cepat</td>\n",
              "      <td>Positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>layan mana buat suratsurat mudah sistem online bulakbalik penuh dokumen langsung cetak mandiri bawa ojek online tugas ramah bimbing sulit moga layan tahan tingkat terimakasih</td>\n",
              "      <td>Positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gaji staf staf dukcapil kerja buru buru pulang jam salah salah cetak kartu tanda duduk kartu keluarga status agama kartu tanda duduk ganti buru buru pulang kerja beres</td>\n",
              "      <td>Negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>proses cepat online riweh bingung tinggal admin layan ramah ya beda sangtta yaa sih ter the best pokok</td>\n",
              "      <td>Positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                       stemmed_text   \n",
              "0                                                                                                                                                                     urus kartu tanda duduk hilang surat terang hilang polisi fotokopi kartu keluarga antri jam proses cetak menit  \\\n",
              "1  urus akta nikah online taring proses cetak arah kantor informasi terima email cetak akta paket kartu tanda duduk kartu keluarga terbit admin taring respon tanggap cepat respect admin suasana dukcapil loket layan cepat ramah tunggu nyaman antri ramai paham alur mudah cepat   \n",
              "2                                                                                                    layan mana buat suratsurat mudah sistem online bulakbalik penuh dokumen langsung cetak mandiri bawa ojek online tugas ramah bimbing sulit moga layan tahan tingkat terimakasih   \n",
              "3                                                                                                           gaji staf staf dukcapil kerja buru buru pulang jam salah salah cetak kartu tanda duduk kartu keluarga status agama kartu tanda duduk ganti buru buru pulang kerja beres   \n",
              "4                                                                                                                                                                            proses cepat online riweh bingung tinggal admin layan ramah ya beda sangtta yaa sih ter the best pokok   \n",
              "\n",
              "  sentiment_category  encoded_label  \n",
              "0           Positive              2  \n",
              "1           Positive              2  \n",
              "2           Positive              2  \n",
              "3           Negative              0  \n",
              "4           Positive              2  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Custom label mapping\n",
        "label_mapping = {1: 'Negative', 2: 'Negative', 3: 'Neutral', 4: 'Positive', 5: 'Positive'}\n",
        "\n",
        "# Create a new DataFrame with only the relevant columns\n",
        "df_encoded = pd.DataFrame()\n",
        "df_encoded['stemmed_text'] = dfnew['stemmed_text']\n",
        "\n",
        "# Map labels to the custom categories\n",
        "df_encoded['sentiment_category'] = dfnew['review_rating'].map(label_mapping)\n",
        "\n",
        "# Handle any remaining NaN values (replace with a default value, e.g., 'Neutral')\n",
        "df_encoded['sentiment_category'].fillna('Neutral', inplace=True)\n",
        "\n",
        "# Now you can proceed with label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "df_encoded['encoded_label'] = label_encoder.fit_transform(df_encoded['sentiment_category'])\n",
        "\n",
        "# Print the new DataFrame to inspect\n",
        "df_encoded.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ueAhziHNci4_"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_encoded['stemmed_text'], df_encoded['encoded_label'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Alql-mGZgUQT"
      },
      "outputs": [],
      "source": [
        "max_words = 10000\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "max_length = 100  # Adjust based on your dataset\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ld5i06DYr9xF",
        "outputId": "f7bd9757-69df-48f8-b17f-8338470bd517"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stemmed_text</th>\n",
              "      <th>sentiment_category</th>\n",
              "      <th>encoded_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>urus kartu tanda duduk hilang surat terang hilang polisi fotokopi kartu keluarga antri jam proses cetak menit</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>urus akta nikah online taring proses cetak arah kantor informasi terima email cetak akta paket kartu tanda duduk kartu keluarga terbit admin taring respon tanggap cepat respect admin suasana dukcapil loket layan cepat ramah tunggu nyaman antri ramai paham alur mudah cepat</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>layan mana buat suratsurat mudah sistem online bulakbalik penuh dokumen langsung cetak mandiri bawa ojek online tugas ramah bimbing sulit moga layan tahan tingkat terimakasih</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gaji staf staf dukcapil kerja buru buru pulang jam salah salah cetak kartu tanda duduk kartu keluarga status agama kartu tanda duduk ganti buru buru pulang kerja beres</td>\n",
              "      <td>Negative</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>proses cepat online riweh bingung tinggal admin layan ramah ya beda sangtta yaa sih ter the best pokok</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                       stemmed_text   \n",
              "0                                                                                                                                                                     urus kartu tanda duduk hilang surat terang hilang polisi fotokopi kartu keluarga antri jam proses cetak menit  \\\n",
              "1  urus akta nikah online taring proses cetak arah kantor informasi terima email cetak akta paket kartu tanda duduk kartu keluarga terbit admin taring respon tanggap cepat respect admin suasana dukcapil loket layan cepat ramah tunggu nyaman antri ramai paham alur mudah cepat   \n",
              "2                                                                                                    layan mana buat suratsurat mudah sistem online bulakbalik penuh dokumen langsung cetak mandiri bawa ojek online tugas ramah bimbing sulit moga layan tahan tingkat terimakasih   \n",
              "3                                                                                                           gaji staf staf dukcapil kerja buru buru pulang jam salah salah cetak kartu tanda duduk kartu keluarga status agama kartu tanda duduk ganti buru buru pulang kerja beres   \n",
              "4                                                                                                                                                                            proses cepat online riweh bingung tinggal admin layan ramah ya beda sangtta yaa sih ter the best pokok   \n",
              "\n",
              "  sentiment_category  encoded_label  \n",
              "0           Positive            0.0  \n",
              "1           Positive            0.0  \n",
              "2           Positive            0.0  \n",
              "3           Negative            1.0  \n",
              "4           Positive            0.0  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Assuming you have a DataFrame df_encoded with 'text' and 'encoded_label' columns\n",
        "# One-hot encode the labels\n",
        "df_encoded['encoded_label'] = to_categorical(df_encoded['encoded_label'])\n",
        "df_encoded.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmNbXPJbsC3A",
        "outputId": "a62c673e-9c3b-49d8-804b-61aa226eb220"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "21/21 [==============================] - 2s 41ms/step - loss: 0.6295 - accuracy: 0.7581 - val_loss: 0.5369 - val_accuracy: 0.7607\n",
            "Epoch 2/15\n",
            "21/21 [==============================] - 1s 26ms/step - loss: 0.4792 - accuracy: 0.8274 - val_loss: 0.4630 - val_accuracy: 0.8589\n",
            "Epoch 3/15\n",
            "21/21 [==============================] - 1s 26ms/step - loss: 0.3371 - accuracy: 0.9060 - val_loss: 0.4021 - val_accuracy: 0.8650\n",
            "Epoch 4/15\n",
            "21/21 [==============================] - 1s 26ms/step - loss: 0.2149 - accuracy: 0.9291 - val_loss: 0.3620 - val_accuracy: 0.8773\n",
            "Epoch 5/15\n",
            "21/21 [==============================] - 1s 25ms/step - loss: 0.1419 - accuracy: 0.9615 - val_loss: 0.3473 - val_accuracy: 0.8834\n",
            "Epoch 6/15\n",
            "21/21 [==============================] - 1s 32ms/step - loss: 0.1016 - accuracy: 0.9692 - val_loss: 0.3364 - val_accuracy: 0.8896\n",
            "Epoch 7/15\n",
            "21/21 [==============================] - 1s 29ms/step - loss: 0.0776 - accuracy: 0.9769 - val_loss: 0.3339 - val_accuracy: 0.8834\n",
            "Epoch 8/15\n",
            "21/21 [==============================] - 1s 25ms/step - loss: 0.0598 - accuracy: 0.9861 - val_loss: 0.3359 - val_accuracy: 0.8834\n",
            "Epoch 9/15\n",
            "21/21 [==============================] - 1s 27ms/step - loss: 0.0461 - accuracy: 0.9861 - val_loss: 0.3416 - val_accuracy: 0.8896\n",
            "Epoch 10/15\n",
            "21/21 [==============================] - 1s 27ms/step - loss: 0.0379 - accuracy: 0.9892 - val_loss: 0.3452 - val_accuracy: 0.8896\n",
            "Epoch 11/15\n",
            "21/21 [==============================] - 1s 28ms/step - loss: 0.0312 - accuracy: 0.9908 - val_loss: 0.3500 - val_accuracy: 0.8896\n",
            "Epoch 12/15\n",
            "21/21 [==============================] - 1s 25ms/step - loss: 0.0264 - accuracy: 0.9969 - val_loss: 0.3667 - val_accuracy: 0.8712\n",
            "Epoch 13/15\n",
            "21/21 [==============================] - 1s 25ms/step - loss: 0.0250 - accuracy: 0.9923 - val_loss: 0.3779 - val_accuracy: 0.8712\n",
            "Epoch 14/15\n",
            "21/21 [==============================] - 1s 27ms/step - loss: 0.0203 - accuracy: 0.9969 - val_loss: 0.3740 - val_accuracy: 0.8528\n",
            "Epoch 15/15\n",
            "21/21 [==============================] - 1s 25ms/step - loss: 0.0181 - accuracy: 0.9969 - val_loss: 0.3885 - val_accuracy: 0.8589\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1a0d5989110>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Build a simple neural network\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dense(units=3, activation='softmax'))  # 3 output units for three categories\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_padded, to_categorical(y_train), epochs=15, validation_data=(X_test_padded, to_categorical(y_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKN3WhEB5r-_",
        "outputId": "b686d71f-efac-4aa3-d4d1-2c5dda66d7f9"
      },
      "outputs": [],
      "source": [
        "#Save the model and tokenizer\n",
        "model.save('sentiment_model.h5')\n",
        "tokenizer_config = tokenizer.get_config()\n",
        "tokenizer_config['num_words'] = max_words\n",
        "with open('tokenizer_config.json', 'w') as config_file:\n",
        "    config_file.write(json.dumps(tokenizer_config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "o0YWYW1oAFPx"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "# ...\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = models.load_model('sentiment_model.h5')\n",
        "with open('tokenizer_config.json') as config_file:\n",
        "    config = json.load(config_file)\n",
        "    max_words = config['num_words']\n",
        "    tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "    tokenizer.word_index = json.loads(config['word_index'])\n",
        "    tokenizer.index_word = {str(i): word for word, i in tokenizer.word_index.items()}\n",
        "\n",
        "# ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YHbKTa9dCrel"
      },
      "outputs": [],
      "source": [
        "app = Flask(__name__)\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    if request.method == 'POST':\n",
        "        text = request.form['text']\n",
        "        result = perform_sentiment_analysis(text)\n",
        "        return render_template('index.html', result=result, text=text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "r2ha1AYNDHjA"
      },
      "outputs": [],
      "source": [
        "def perform_sentiment_analysis(text):\n",
        "    # Preprocess the input text\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "\n",
        "    # Tokenize and pad the sequence\n",
        "    max_length = 100  # Adjust based on your dataset\n",
        "    text_seq = tokenizer.texts_to_sequences([preprocessed_text])\n",
        "    text_padded = pad_sequences(text_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(text_padded)\n",
        "    sentiment = 'Positive' if prediction > 0.5 else 'Negative'\n",
        "\n",
        "    return sentiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "t3-73NhyDKQd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
            " * Running on http://127.0.0.1:5000\n",
            "Press CTRL+C to quit\n",
            " * Restarting with watchdog (windowsapi)\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "1",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Diputra_W\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D575p0RPEpMf"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, render_template, request\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import json  # Import the json module\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the data from the Excel file\n",
        "df = pd.read_excel('your_file.xlsx')  # Replace 'your_file.xlsx' with the actual path to your Excel file\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removing punctuation\n",
        "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
        "\n",
        "    # Removing stopwords\n",
        "\n",
        "    return text\n",
        "\n",
        "df['text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Encode labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize and pad the sequences\n",
        "max_words = 10000  # Adjust based on your dataset\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "max_length = 100  # Adjust based on your dataset\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "# Build a simple neural network\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_padded, y_train, epochs=5, validation_data=(X_test_padded, y_test))\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save('sentiment_model.h5')\n",
        "tokenizer_config = tokenizer.get_config()\n",
        "tokenizer_config['num_words'] = max_words\n",
        "with open('tokenizer_config.json', 'w') as config_file:\n",
        "    config_file.write(json.dumps(tokenizer_config))\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = load_model('sentiment_model.h5')\n",
        "with open('tokenizer_config.json') as config_file:\n",
        "    config = json.load(config_file)\n",
        "    tokenizer = Tokenizer.from_config(config)\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    if request.method == 'POST':\n",
        "        text = request.form['text']\n",
        "        result = perform_sentiment_analysis(text)\n",
        "        return render_template('index.html', result=result, text=text)\n",
        "\n",
        "def perform_sentiment_analysis(text):\n",
        "    # Preprocess the input text\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "\n",
        "    # Tokenize and pad the sequence\n",
        "    max_length = 100  # Adjust based on your dataset\n",
        "    text_seq = tokenizer.texts_to_sequences([preprocessed_text])\n",
        "    text_padded = pad_sequences(text_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(text_padded)\n",
        "    sentiment = 'Positive' if prediction > 0.5 else 'Negative'\n",
        "\n",
        "    return sentiment\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
